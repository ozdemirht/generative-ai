# RAG with Books

## Introduction

## Pipeline

### Embedding
OpenAI - model selection trade-off cost - accuracy
Local - model selection trade-off latency - accuracy

Very large documents ingestion via streaming 

### Memory/Context Maintenance
Local -
Claude -  

### Submitting a user query

#### Select relevant data from Vector Database by using a user query
- Maximum Marginal Reference (retrieve diverse context document chunks)
- Compression (ContextualCompressionRetriever)
- SelfQueryRetriever
#### Update Memory if needed

#### Invoke with context and user query

## References
